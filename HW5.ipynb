{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем отызывы за лето (из архива с материалами или предыдущего занятия)\n",
    "1. Учим conv сеть для классификации - выбить auc выше 0.95\n",
    "2. Предобучаем word2vec и его эмбединга инициализируем сетку, как влияет на качество?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from stop_words import get_stop_words\n",
    "from string import punctuation\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D, GlobalMaxPool1D, AveragePooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard \n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.callbacks import EarlyStopping  \n",
    "import tensorflow as tf\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Content</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>It just works!</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>В целом удобноное приложение...из минусов хотя...</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Отлично все</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Стал зависать на 1% работы антивируса. Дальше ...</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Очень удобно, работает быстро.</td>\n",
       "      <td>2017-08-14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                            Content        Date\n",
       "0       5                                     It just works!  2017-08-14\n",
       "1       4  В целом удобноное приложение...из минусов хотя...  2017-08-14\n",
       "2       5                                        Отлично все  2017-08-14\n",
       "3       5  Стал зависать на 1% работы антивируса. Дальше ...  2017-08-14\n",
       "4       5                     Очень удобно, работает быстро.  2017-08-14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('отзывы за лето.xls')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 5000\n",
    "max_len = 100\n",
    "num_classes = 1\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 512\n",
    "print_batch_n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)\n",
    "sw = set(get_stop_words(\"ru\"))\n",
    "morpher = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(txt):\n",
    "    txt = str(txt)\n",
    "    txt = txt.lower()\n",
    "    txt = re.sub('https?://\\S+|www\\.\\S+', ' ', txt)\n",
    "    txt = re.sub(r'[^\\w\\s]',' ', txt)\n",
    "    txt = re.sub(r'[0-9]+', ' ', txt)\n",
    "    txt = re.sub('\\n', ' ', txt)\n",
    "    txt = re.sub(\"не\\s\", \"не\", txt)\n",
    "    txt = \"\".join(c for c in txt if c not in exclude)\n",
    "    txt = [morpher.parse(word)[0].normal_form for word in txt.split() if word not in sw]\n",
    "    return \" \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['Content'].apply(preprocess_text)\n",
    "data = data[data['Rating'] != 3]\n",
    "data['target'] = data['Rating'] > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['target'] = data['target'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['target'], test_size=0.2,\n",
    "                                                    random_state=13, stratify=data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = ' '.join(X_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered = [word for word in tokens if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = FreqDist(tokens_filtered)\n",
    "tokens_filtered_top = [pair[0] for pair in dist.most_common(max_words-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {v: k for k, v in dict(enumerate(tokens_filtered_top, 1)).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, maxlen):\n",
    "    result = []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens_filtered = [word for word in tokens if word.isalnum()]\n",
    "    for word in tokens_filtered:\n",
    "        if word in vocabulary:\n",
    "            result.append(vocabulary[word])\n",
    "    padding = [0]*(maxlen-len(result))\n",
    "    return padding + result[-maxlen:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.asarray([text_to_sequence(text, max_len) for text in X_train], dtype=np.int32)\n",
    "x_test = np.asarray([text_to_sequence(text, max_len) for text in X_test], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=512, input_length=max_len))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(loss=loss,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15798 samples, validate on 3950 samples\n",
      "Epoch 1/10\n",
      "15798/15798 [==============================] - 23s 1ms/step - loss: 0.1216 - accuracy: 0.9614 - val_loss: 0.1881 - val_accuracy: 0.9334\n",
      "Epoch 2/10\n",
      "15798/15798 [==============================] - 23s 1ms/step - loss: 0.1132 - accuracy: 0.9638 - val_loss: 0.2103 - val_accuracy: 0.9273\n",
      "Epoch 3/10\n",
      "15798/15798 [==============================] - 23s 1ms/step - loss: 0.1031 - accuracy: 0.9689 - val_loss: 0.2047 - val_accuracy: 0.9289\n",
      "Epoch 4/10\n",
      "15798/15798 [==============================] - 24s 2ms/step - loss: 0.0988 - accuracy: 0.9704 - val_loss: 0.2101 - val_accuracy: 0.9316\n",
      "Epoch 5/10\n",
      "15798/15798 [==============================] - 24s 2ms/step - loss: 0.0919 - accuracy: 0.9723 - val_loss: 0.2100 - val_accuracy: 0.9332\n",
      "Epoch 6/10\n",
      "15798/15798 [==============================] - 24s 2ms/step - loss: 0.0879 - accuracy: 0.9729 - val_loss: 0.2149 - val_accuracy: 0.9296\n",
      "Epoch 7/10\n",
      "15798/15798 [==============================] - 23s 1ms/step - loss: 0.0855 - accuracy: 0.9746 - val_loss: 0.2263 - val_accuracy: 0.9258\n",
      "Epoch 8/10\n",
      "15798/15798 [==============================] - 24s 2ms/step - loss: 0.0810 - accuracy: 0.9756 - val_loss: 0.2097 - val_accuracy: 0.9334\n",
      "Epoch 9/10\n",
      "15798/15798 [==============================] - 23s 1ms/step - loss: 0.0762 - accuracy: 0.9781 - val_loss: 0.2302 - val_accuracy: 0.9289\n",
      "Epoch 10/10\n",
      "15798/15798 [==============================] - 23s 1ms/step - loss: 0.0748 - accuracy: 0.9786 - val_loss: 0.2382 - val_accuracy: 0.9316\n"
     ]
    }
   ],
   "source": [
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3950/3950 [==============================] - 1s 252us/step\n",
      "15798/15798 [==============================] - 5s 331us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_val, batch_size=batch_size, verbose=1)\n",
    "score_train = model.evaluate(x_train, y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.23823399681833726 \tTrain score:  0.0678795961523293\n",
      "Test accuracy: 0.9316455721855164 \tTrain accuracy:  0.9829725027084351\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score[0], '\\tTrain score: ', score_train[0])\n",
    "print('Test accuracy:', score[1], '\\tTrain accuracy: ', score_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in ([[sentence] for sentence in data.text.tolist()]):\n",
    "    corpus.append(i[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(corpus, min_count = 5, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['it', 'just', 'works'],\n",
       " ['целое',\n",
       "  'удобноной',\n",
       "  'приложение',\n",
       "  'минус',\n",
       "  'хотеть',\n",
       "  'большой',\n",
       "  'доступ',\n",
       "  'персональный',\n",
       "  'данные',\n",
       "  'телефонеприходиться',\n",
       "  'пользоваться',\n",
       "  'ограниченный',\n",
       "  'режим'],\n",
       " ['отлично'],\n",
       " ['зависать', 'работа', 'антивирус', 'ранее', 'пользоваться', 'нормальный'],\n",
       " ['удобно', 'работать', 'быстро']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('запускаться', 0.999802827835083),\n",
       " ('сервер', 0.9997774362564087),\n",
       " ('долгий', 0.9997384548187256),\n",
       " ('происходить', 0.99969881772995),\n",
       " ('каждый', 0.9996768236160278),\n",
       " ('час', 0.999667763710022),\n",
       " ('грузиться', 0.9996631741523743),\n",
       " ('постоянный', 0.9996595978736877),\n",
       " ('ужасно', 0.9996515512466431),\n",
       " ('бесить', 0.9996505975723267)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('зависать')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_ebm(txt):\n",
    "    summ_ = np.zeros(100)\n",
    "    for word in txt.split():\n",
    "        if word in model.wv:\n",
    "            summ_ += model.wv[word]\n",
    "    return summ_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb = pd.DataFrame(X_train)\n",
    "X_test_emb = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_emb['sum_emb'] = X_train_emb.text.apply(summ_ebm)\n",
    "X_test_emb['sum_emb'] = X_test_emb.text.apply(summ_ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_emb = np.zeros((X_train_emb.shape[0], 100))\n",
    "xtest_emb = np.zeros((X_train_emb.shape[0], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_train_emb.shape[0]):\n",
    "    xtrain_emb[i] = X_train_emb.iloc[i].sum_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(X_test_emb.shape[0]):\n",
    "    xtest_emb[i] = X_test_emb.iloc[i].sum_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_len, weights=[xtrain_emb[:max_words]]))\n",
    "model.add(Conv1D(128, 3))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy()\n",
    "model.compile(loss=loss,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15798 samples, validate on 3950 samples\n",
      "Epoch 1/20\n",
      "15798/15798 [==============================] - 7s 451us/step - loss: 0.6052 - accuracy: 0.8459 - val_loss: 0.4147 - val_accuracy: 0.8524\n",
      "Epoch 2/20\n",
      "15798/15798 [==============================] - 7s 442us/step - loss: 0.4440 - accuracy: 0.8808 - val_loss: 0.4094 - val_accuracy: 0.8678\n",
      "Epoch 3/20\n",
      "15798/15798 [==============================] - 7s 435us/step - loss: 0.3639 - accuracy: 0.8806 - val_loss: 0.3484 - val_accuracy: 0.8876\n",
      "Epoch 4/20\n",
      "15798/15798 [==============================] - 7s 457us/step - loss: 0.3355 - accuracy: 0.8972 - val_loss: 0.3252 - val_accuracy: 0.8906\n",
      "Epoch 5/20\n",
      "15798/15798 [==============================] - 7s 439us/step - loss: 0.3133 - accuracy: 0.9047 - val_loss: 0.3072 - val_accuracy: 0.8954\n",
      "Epoch 6/20\n",
      "15798/15798 [==============================] - 7s 469us/step - loss: 0.2846 - accuracy: 0.9092 - val_loss: 0.2916 - val_accuracy: 0.9058\n",
      "Epoch 7/20\n",
      "15798/15798 [==============================] - 7s 439us/step - loss: 0.2706 - accuracy: 0.9189 - val_loss: 0.2899 - val_accuracy: 0.9109\n",
      "Epoch 8/20\n",
      "15798/15798 [==============================] - 7s 456us/step - loss: 0.9503 - accuracy: 0.6450 - val_loss: 0.4826 - val_accuracy: 0.8939\n",
      "Epoch 9/20\n",
      "15798/15798 [==============================] - 7s 435us/step - loss: 0.3654 - accuracy: 0.9067 - val_loss: 0.3328 - val_accuracy: 0.9091\n",
      "Epoch 10/20\n",
      "15798/15798 [==============================] - 7s 444us/step - loss: 0.3127 - accuracy: 0.9160 - val_loss: 0.3175 - val_accuracy: 0.9089\n",
      "Epoch 11/20\n",
      "15798/15798 [==============================] - 7s 449us/step - loss: 0.2971 - accuracy: 0.9144 - val_loss: 0.3048 - val_accuracy: 0.9041\n",
      "Epoch 12/20\n",
      "15798/15798 [==============================] - 7s 437us/step - loss: 0.2787 - accuracy: 0.9258 - val_loss: 0.3082 - val_accuracy: 0.9144\n",
      "Epoch 13/20\n",
      "15798/15798 [==============================] - 7s 454us/step - loss: 0.3257 - accuracy: 0.9246 - val_loss: 0.4446 - val_accuracy: 0.9165\n",
      "Epoch 14/20\n",
      "15798/15798 [==============================] - 7s 435us/step - loss: 0.3289 - accuracy: 0.9282 - val_loss: 0.3038 - val_accuracy: 0.9122\n",
      "Epoch 15/20\n",
      "15798/15798 [==============================] - 7s 453us/step - loss: 0.2445 - accuracy: 0.9304 - val_loss: 0.2846 - val_accuracy: 0.9152\n",
      "Epoch 16/20\n",
      "15798/15798 [==============================] - 7s 436us/step - loss: 0.2346 - accuracy: 0.9344 - val_loss: 0.2810 - val_accuracy: 0.9200\n",
      "Epoch 17/20\n",
      "15798/15798 [==============================] - 7s 452us/step - loss: 0.2386 - accuracy: 0.9387 - val_loss: 0.3158 - val_accuracy: 0.9177\n",
      "Epoch 18/20\n",
      "15798/15798 [==============================] - 7s 437us/step - loss: 0.2305 - accuracy: 0.9383 - val_loss: 0.2800 - val_accuracy: 0.9197\n",
      "Epoch 19/20\n",
      "15798/15798 [==============================] - 7s 440us/step - loss: 0.2820 - accuracy: 0.9300 - val_loss: 0.3693 - val_accuracy: 0.9228\n",
      "Epoch 20/20\n",
      "15798/15798 [==============================] - 7s 454us/step - loss: 0.2948 - accuracy: 0.9334 - val_loss: 0.2815 - val_accuracy: 0.9147\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3950/3950 [==============================] - 0s 78us/step\n",
      "15798/15798 [==============================] - 2s 96us/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_val, batch_size=batch_size, verbose=1)\n",
    "score_train = model.evaluate(x_train, y_train, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.28148891538004334 \tTrain score:  0.22062032195054546\n",
      "Test accuracy: 0.9146835207939148 \tTrain accuracy:  0.9353082776069641\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score[0], '\\tTrain score: ', score_train[0])\n",
    "print('Test accuracy:', score[1], '\\tTrain accuracy: ', score_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
